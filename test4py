import cv2
import numpy as np
from tensorflow.keras.models import load_model

# Funzione di Non-Maximum Suppression per unire bounding box sovrapposti
def non_max_suppression_fast(boxes, overlapThresh=0.3):
    if len(boxes) == 0:
        return []

    # Converte i box in un array numpy, formato [x1, y1, x2, y2]
    boxes = np.array(boxes, dtype="float")
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 0] + boxes[:, 2]
    y2 = boxes[:, 1] + boxes[:, 3]

    # Calcola l'area di ogni box
    area = (x2 - x1 + 1) * (y2 - y1 + 1)
    # Ordina per coordinata di fine (qui si può anche ordinare per area o confidenza, se disponibile)
    idxs = np.argsort(y2)

    pick = []
    while len(idxs) > 0:
        # Seleziona l'ultimo indice, quello con il più alto valore di y2
        last = len(idxs) - 1
        i = idxs[last]
        pick.append(i)
        
        # Trova le coordinate dell'intersezione tra il box corrente e quelli rimanenti
        xx1 = np.maximum(x1[i], x1[idxs[:last]])
        yy1 = np.maximum(y1[i], y1[idxs[:last]])
        xx2 = np.minimum(x2[i], x2[idxs[:last]])
        yy2 = np.minimum(y2[i], y2[idxs[:last]])
        
        # Calcola le aree dell'intersezione e il rapporto di sovrapposizione (IoU)
        w = np.maximum(0, xx2 - xx1 + 1)
        h = np.maximum(0, yy2 - yy1 + 1)
        overlap = (w * h) / area[idxs[:last]]
        
        # Elimina gli indici in cui l'overlap è superiore alla soglia
        idxs = np.delete(
            idxs, 
            np.concatenate(([last], np.where(overlap > overlapThresh)[0]))
        )
    
    # Restituisce le ROI finali nel formato originale [x, y, w, h]
    final_boxes = []
    for i in pick:
        final_boxes.append([int(x1[i]), int(y1[i]), int(x2[i]-x1[i]), int(y2[i]-y1[i])])
    return final_boxes

# Carica il modello addestrato
model = load_model("BlocksWorld_2425/modellone.keras")
# Carica l'immagine
image = cv2.imread('BlocksWorld_2425/test_immagini/scenaTelefono1.jpeg')

# --- Pre-elaborazione dell'immagine ---
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
denoised = cv2.fastNlMeansDenoising(gray, None, h=15, templateWindowSize=7, searchWindowSize=21)
equalized = cv2.equalizeHist(denoised)
blurred = cv2.GaussianBlur(equalized, (5, 5), 0)
thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                               cv2.THRESH_BINARY_INV, 11, 2)

kernel = np.ones((3, 3), np.uint8)
closing = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=1)
thresh_clean = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel, iterations=1)

# Trova i contorni
contours, _ = cv2.findContours(thresh_clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
min_area = 150
boxes = []
for cnt in contours:
    if cv2.contourArea(cnt) < min_area:
        continue
    x, y, w, h = cv2.boundingRect(cnt)
    boxes.append([x, y, w, h])

# Applica Non-Maximum Suppression per unire box sovrapposti
merged_boxes = non_max_suppression_fast(boxes, overlapThresh=0.3)

# --- Elaborazione e classificazione delle ROI ottenute ---
for box in merged_boxes:
    x, y, w, h = box
    # Riduci il padding se stai includendo troppo contesto
    padding = int(0.1 * min(w, h))
    x_start = max(x - padding, 0)
    y_start = max(y - padding, 0)
    x_end = min(x + w + padding, image.shape[1])
    y_end = min(y + h + padding, image.shape[0])
    
    # Estrai l'area ROI dalla maschera pulita (o dall'immagine elaborata)
    roi = thresh_clean[y_start:y_end, x_start:x_end]
    
    # Fine cropping: trova i pixel significativi e ritaglia ulteriormente
    non_zero_points = cv2.findNonZero(roi)
    if non_zero_points is not None:
        x2, y2, w2, h2 = cv2.boundingRect(non_zero_points)
        roi = roi[y2:y2+h2, x2:x2+w2]
    
    # Rendi l'immagine quadrata: aggiungi padding per centrare il contenuto
    roi_h, roi_w = roi.shape
    if roi_w > roi_h:
        diff = roi_w - roi_h
        top = diff // 2
        bottom = diff - top
        roi_square = cv2.copyMakeBorder(roi, top, bottom, 0, 0, cv2.BORDER_CONSTANT, value=0)
    elif roi_h > roi_w:
        diff = roi_h - roi_w
        left = diff // 2
        right = diff - left
        roi_square = cv2.copyMakeBorder(roi, 0, 0, left, right, cv2.BORDER_CONSTANT, value=0)
    else:
        roi_square = roi
    
    roi_height, roi_width = roi_square.shape
    # Calcola un margine proporzionale (ad esempio, il 10% della dimensione minore)
    margin = int(0.1 * min(roi_height, roi_width))
    
    # Ritaglia la ROI usando il margine dinamico
    roi_cropped = roi_square[margin:roi_height-margin, margin:roi_width-margin]
    cv2.imshow("ROI cropped", roi_cropped)
    cv2.waitKey(0)
    
    # Ridimensiona la ROI a 28x28 (lo standard per MNIST)
    roi_resized = cv2.resize(roi_cropped, (28, 28), interpolation=cv2.INTER_AREA)
    roi_normalized = roi_resized.astype("float32") / 255.0
    roi_input = roi_normalized.reshape(1, 28, 28, 1)
    
    # Effettua la previsione
    prediction = model.predict(roi_input)
    confidence = np.max(prediction)
    predicted_digit = np.argmax(prediction)
    
    # Puoi decidere di scartare previsione a bassa confidenza
    if confidence < 0.6:
        continue
    
    # Disegna il bounding box (originale o rivisto) e il risultato sull'immagine
    cv2.rectangle(image, (x_start, y_start), (x_end, y_end), (0, 255, 0), 2)
    cv2.putText(image, str(predicted_digit), (x_start, y_start - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

# Visualizza il risultato
cv2.imshow("Risultato", cv2.resize(image, (800, 800), interpolation=cv2.INTER_LINEAR))
cv2.imshow("Thresh Clean", thresh_clean)
cv2.waitKey(0)
cv2.destroyAllWindows()
